{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bc9c84b",
   "metadata": {},
   "source": [
    "# DeepDAP：Deep learning-assisted to accelerate the discovery of donor/acceptor pairs for high-performance organic solar cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcf8b2d",
   "metadata": {},
   "source": [
    "It is a deep learning-based framework built for new donor/acceptor pairs (DeepDAP) discovery. The framework contains data collection section, PCE prediction section and molecular discovery section. Specifically, a large D/A pair dataset was built by collecting experimental data from literature. Then, a novel RoBERTa-based dual-encoder model (DeRoBERTa) was developed for PCE prediction by using the SMILES of donor and acceptor pairs as the input. Two pretrained ChemBERTa2 encoders were loaded as initial parameters of the dual-encoder. The model was trained, tested and validated on the experimental dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5fa729",
   "metadata": {},
   "source": [
    "## Here, we have shown how to use the model to predict the PCE based on the SMILES of donor and acceptors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80abc44b",
   "metadata": {},
   "source": [
    "### 1. Download the trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c6658dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py): started\n",
      "  Building wheel for wget (setup.py): finished with status 'done'\n",
      "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9657 sha256=bbe0bf5cb9300b74075403b78b87a86314ebb304aff57056f38fc4dcb6bd6fca\n",
      "  Stored in directory: C:\\Users\\BM109X32G-10GPU-02\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-v97hsxon\\wheels\\bd\\a8\\c3\\3cf2c14a1837a4e04bd98631724e81f33f462d86a1d895fae0\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003e8fee",
   "metadata": {},
   "source": [
    "#### download the prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4c2aba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "url = r\"https://github.com/JinYSun/DeepDAP/releases/download/v1.0.0/test.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d925e01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [........................................................................] 81596523 / 81596523"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'DeepDAP/OSC/test (1).ckpt'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wget.download(url,\"DeepDAP/OSC/test.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0109f8d",
   "metadata": {},
   "source": [
    "### It is recommended to retrain and calculate on the supercomputing!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9268bec",
   "metadata": {},
   "source": [
    "### 2. Predict For large-scale screening"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a50990",
   "metadata": {},
   "source": [
    "put the dataset containing the SMILES of donors and acceptors in dataset/OSC/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fcaf781",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepChem/ChemBERTa-10M-MLM were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-10M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at DeepChem/ChemBERTa-10M-MTR were not used when initializing RobertaModel: ['norm_mean', 'regression.out_proj.weight', 'regression.out_proj.bias', 'regression.dense.bias', 'regression.dense.weight', 'norm_std']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-10M-MTR and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at DeepChem/ChemBERTa-10M-MLM were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-10M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at DeepChem/ChemBERTa-10M-MTR were not used when initializing RobertaModel: ['norm_mean', 'regression.out_proj.weight', 'regression.out_proj.bias', 'regression.dense.bias', 'regression.dense.weight', 'norm_std']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-10M-MTR and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "E:\\anaconda\\lib\\site-packages\\pytorch_lightning\\core\\saving.py:251: UserWarning: Found keys that are in the model state dict but not in the checkpoint: ['d_model.embeddings.position_ids', 'p_model.embeddings.position_ids']\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "from DeepDAP import screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd77adb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:19<00:00,  4.83s/it]\n"
     ]
    }
   ],
   "source": [
    "x = screen.smiles_aas_test( r\"DeepDAP\\dataset\\OSC\\test.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53064046",
   "metadata": {},
   "source": [
    "### 3. Predict the PCE by using D/A pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "216856d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepChem/ChemBERTa-10M-MLM were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-10M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at DeepChem/ChemBERTa-10M-MTR were not used when initializing RobertaModel: ['norm_mean', 'regression.out_proj.weight', 'regression.out_proj.bias', 'regression.dense.bias', 'regression.dense.weight', 'norm_std']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-10M-MTR and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "E:\\anaconda\\lib\\site-packages\\pytorch_lightning\\core\\saving.py:251: UserWarning: Found keys that are in the model state dict but not in the checkpoint: ['d_model.embeddings.position_ids', 'p_model.embeddings.position_ids']\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "from DeepDAP import run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31d1da1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.416102886199951\n"
     ]
    }
   ],
   "source": [
    "a = run.smiles_adp_test('CCCCC(CC)CC1=C(F)C=C(C2=C3C=C(C4=CC=C(C5=C6C(=O)C7=C(CC(CC)CCCC)SC(CC(CC)CCCC)=C7C(=O)C6=C(C6=CC=C(C)S6)S5)S4)SC3=C(C3=CC(F)=C(CC(CC)CCCC)S3)C3=C2SC(C)=C3)S1','CCCCC(CC)CC1=CC=C(C2=C3C=C(C)SC3=C(C3=CC=C(CC(CC)CCCC)S3)C3=C2SC(C2=CC4=C(C5=CC(Cl)=C(CC(CC)CCCC)S5)C5=C(C=C(C)S5)C(C5=CC(Cl)=C(CC(CC)CCCC)S5)=C4S2)=C3)S1')                         \n",
    "print(a)\n",
    "                     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7e610c",
   "metadata": {},
   "source": [
    "## Acknowledgement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c325417d",
   "metadata": {},
   "source": [
    "Jinyu Sun \n",
    "\n",
    "E-mail: jinyusun@csu.edu.cn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
