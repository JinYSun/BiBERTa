{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bc9c84b",
   "metadata": {},
   "source": [
    "# BiBERTa：Deep learning-assisted to accelerate the discovery of donor/acceptor pairs for high-performance organic solar cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcf8b2d",
   "metadata": {},
   "source": [
    "It is a deep learning-based framework built for new donor/acceptor pairs discovery. The framework contains data collection section, PCE prediction section and molecular discovery section. Specifically, a large D/A pair dataset was built by collecting experimental data from literature. Then, a novel RoBERTa-based dual-encoder model (BiBERTa) was developed for PCE prediction by using the SMILES of donor and acceptor pairs as the input. Two pretrained ChemBERTa2 encoders were loaded as initial parameters of the dual-encoder. The model was trained, tested and validated on the experimental dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ad1c6a",
   "metadata": {},
   "source": [
    "It's an example for the whole process. \n",
    "It was used to test that the code works. \n",
    "All parameters were set ##small## to show how the BiBERTa worked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d35032f",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb133d98",
   "metadata": {},
   "source": [
    "The BiBERTa contains bi-RoBERTa encoder layers and interaction layers. The SMILES of donor and acceptor pairs are used as the input of the model. Two pre-trained ChemBERTa2 encoders are loaded as initial parameters of the dual-encoder layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ef66dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7200f1ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 111\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Some weights of the model checkpoint at DeepChem/ChemBERTa-10M-MTR were not used when initializing RobertaModel: ['regression.out_proj.bias', 'norm_std', 'norm_mean', 'regression.dense.bias', 'regression.dense.weight', 'regression.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-10M-MTR and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at DeepChem/ChemBERTa-10M-MLM were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-10M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "E:\\anaconda\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:613: UserWarning: Checkpoint directory OSC_TrueToTrue_0.0001_111 exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name             | Type         | Params\n",
      "--------------------------------------------------\n",
      "0 | criterion        | MSELoss      | 0     \n",
      "1 | criterion_smooth | SmoothL1Loss | 0     \n",
      "2 | d_model          | RobertaModel | 3.4 M \n",
      "3 | p_model          | RobertaModel | 3.4 M \n",
      "4 | decoder          | Sequential   | 131 K \n",
      "--------------------------------------------------\n",
      "7.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.0 M     Total params\n",
      "27.946    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mae : 11.320357322692871\n",
      "mse : 129.50091552734375\n",
      "r2 : -95.02465836638983\n",
      "r : (-0.15681785877868776, 0.8997528739238635)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d28ce9e9e34f6b83036bd7e67e1433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mae : 11.339509010314941\n",
      "mse : 129.9381866455078\n",
      "r2 : -95.34889210142464\n",
      "r : (-0.695970444850101, 0.5099488927664776)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mae : 11.353251457214355\n",
      "mse : 130.25604248046875\n",
      "r2 : -95.58458531057506\n",
      "r : (-0.8837593217927395, 0.3100085823443737)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mae : 11.365836143493652\n",
      "mse : 130.54246520996094\n",
      "r2 : -95.7969671662234\n",
      "r : (-0.9291022063013242, 0.24116338773809162)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mae : 11.378275871276855\n",
      "mse : 130.8253936767578\n",
      "r2 : -96.00675803338187\n",
      "r : (-0.9387377555338279, 0.22399279977766837)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mae : 11.391215324401855\n",
      "mse : 131.12200927734375\n",
      "r2 : -96.2267016531091\n",
      "r : (-0.9474233972244441, 0.20735418430665573)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mae : 11.403160095214844\n",
      "mse : 131.39552307128906\n",
      "r2 : -96.42950967428648\n",
      "r : (-0.970118117214374, 0.15602224944152876)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mae : 11.41584300994873\n",
      "mse : 131.6869659423828\n",
      "r2 : -96.64561206871977\n",
      "r : (-0.9881986259950967, 0.09790152141628687)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mae : 11.426310539245605\n",
      "mse : 131.9269561767578\n",
      "r2 : -96.82356621195204\n",
      "r : (-0.9990359187946117, 0.027956759244627788)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mae : 11.432506561279297\n",
      "mse : 132.0695343017578\n",
      "r2 : -96.92928947955389\n",
      "r : (-0.993156121959623, 0.07452367294514158)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mae : 11.436820030212402\n",
      "mse : 132.1690673828125\n",
      "r2 : -97.00308362262108\n",
      "r : (-0.9875166330838923, 0.10069636984177498)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mae : 11.438968658447266\n",
      "mse : 132.21923828125\n",
      "r2 : -97.04029466765073\n",
      "r : (-0.9831523276243125, 0.11702447589363633)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mae : 11.44015121459961\n",
      "mse : 132.2470245361328\n",
      "r2 : -97.06088868897383\n",
      "r : (-0.9853580435809046, 0.10907519901164801)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=12` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "E:\\anaconda\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1094273eccfb4767aec7c264f00df4d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "mae : 11.44015121459961\n",
      "mse : 132.2470245361328\n",
      "r2 : -97.06088868897383\n",
      "r : (-0.9853580435809046, 0.10907519901164801)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">            mae            </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     11.44015121459961     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">            mse            </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     132.2470245361328     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">            r2             </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    -97.06088868897383     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    132.24700927734375     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m           mae           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    11.44015121459961    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m           mse           \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    132.2470245361328    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m           r2            \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   -97.06088868897383    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   132.24700927734375    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train.main(using_wandb = False, hparams = 'config/config_hparam.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9268bec",
   "metadata": {},
   "source": [
    "## Screen for large-scale dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fcaf781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd77adb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  6.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'acceptor': 'CCCCCCCCC1(CCCCCCCC)c2cc3c(cc2-c2cc4c(cc21)-c1sc(/C=C2\\\\C(=O)c5ccccc5C2=C(C#N)C#N)cc1C4(CCCCCCCC)CCCCCCCC)C(CCCCCCCC)(CCCCCCCC)c1cc(/C=C2\\\\C(=O)c4ccccc4C2=C(C#N)C#N)sc1-3', 'donor': 'CCCCC(CC)Cc1sc(-c2c3cc(-c4ccc(-c5sc(-c6ccc(C)s6)c6c5C(=O)c5c(CC(CC)CCCC)sc(CC(CC)CCCC)c5C6=O)s4)sc3c(-c3cc(F)c(CC(CC)CCCC)s3)c3cc(C)sc23)cc1F', 'predict': 10.974780082702637}, {'acceptor': 'CCCCCCc1ccc(C2(c3ccc(CCCCCC)cc3)c3cc4c(cc3-c3sc5cc(/C=C6\\\\C(=O)c7ccccc7C6=C(C#N)C#N)sc5c32)C(c2ccc(CCCCCC)cc2)(c2ccc(CCCCCC)cc2)c2c-4sc3cc(/C=C4\\\\C(=O)c5ccccc5C4=C(C#N)C#N)sc23)cc1', 'donor': 'CCCCCCCCOc1cccc(-c2nc3c(-c4ccc(C)s4)c(F)c(F)c(-c4ccc(-c5cc6c(-c7cc(F)c(CC(CC)CCCC)s7)c7sc(C)cc7c(-c7cc(F)c(CC(CC)CCCC)s7)c6s5)s4)c3nc2-c2cccc(OCCCCCCCC)c2)c1', 'predict': 8.40988540649414}, {'acceptor': 'CCCCCCc1ccc(C2(c3ccc(CCCCCC)cc3)c3c(sc4cc(/C=C5\\\\C(=O)c6cc(F)c(F)cc6C5=C(C#N)C#N)sc34)-c3sc4c5c(sc4c32)-c2sc3cc(/C=C4\\\\C(=O)c6cc(F)c(F)cc6C4=C(C#N)C#N)sc3c2C5(c2ccc(CCCCCC)cc2)c2ccc(CCCCCC)cc2)cc1', 'donor': 'CCCCC(CC)COC(=O)c1sc2c(C)sc(-c3ccc(-c4sc(-c5ccc(-c6sc(-c7cc8c(-c9cc(Cl)c(CC(CC)CCCC)s9)c9sc(C)cc9c(-c9cc(Cl)c(CC(CC)CCCC)s9)c8s7)c7sc(C(=O)OCC(CC)CCCC)c(F)c67)s5)c5c4C(=O)c4c(CC(CC)CCCC)sc(CC(CC)CCCC)c4C5=O)s3)c2c1F', 'predict': 10.442777633666992}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x = screen.smiles_aas_test( r\"dataset\\OSC\\test.csv\")\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb8426c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f47e0d18",
   "metadata": {},
   "source": [
    "## Predict by using D/A pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33916e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepChem/ChemBERTa-10M-MLM were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-10M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at DeepChem/ChemBERTa-10M-MTR were not used when initializing RobertaModel: ['regression.out_proj.bias', 'norm_std', 'norm_mean', 'regression.dense.bias', 'regression.dense.weight', 'regression.out_proj.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-10M-MTR and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "E:\\anaconda\\lib\\site-packages\\pytorch_lightning\\core\\saving.py:251: UserWarning: Found keys that are in the model state dict but not in the checkpoint: ['d_model.embeddings.position_ids', 'p_model.embeddings.position_ids']\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "import run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16f9839f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.416102886199951\n"
     ]
    }
   ],
   "source": [
    "a = run.smiles_adp_test('CCCCC(CC)CC1=C(F)C=C(C2=C3C=C(C4=CC=C(C5=C6C(=O)C7=C(CC(CC)CCCC)SC(CC(CC)CCCC)=C7C(=O)C6=C(C6=CC=C(C)S6)S5)S4)SC3=C(C3=CC(F)=C(CC(CC)CCCC)S3)C3=C2SC(C)=C3)S1','CCCCC(CC)CC1=CC=C(C2=C3C=C(C)SC3=C(C3=CC=C(CC(CC)CCCC)S3)C3=C2SC(C2=CC4=C(C5=CC(Cl)=C(CC(CC)CCCC)S5)C5=C(C=C(C)S5)C(C5=CC(Cl)=C(CC(CC)CCCC)S5)=C4S2)=C3)S1')                         \n",
    "print(a)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8c82b507",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d7e610c",
   "metadata": {},
   "source": [
    "## Acknowledgement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c325417d",
   "metadata": {},
   "source": [
    "Jinyu Sun \n",
    "\n",
    "E-mail: jinyusun@csu.edu.cn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
